{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "taxi_reforc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOJcgXvkDg1sley9EVG5sOt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Fonte: https://medium.com/turing-talks/aprendizado-por-refor%C3%A7o-4-gym-d18ac1280628\n",
        "#Este exemplo foi criado por Enzo Cardeal Neves (https://medium.com/@enzocardeal) e foi adaptado por Bruno Menezes (https://github.com/brunoleomenezes) e será utilizado apenas para fins acadêmicos."
      ],
      "metadata": {
        "id": "-K1xe-AsZjJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_SectZMdXhuQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Taxi-v3\").env\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVIsS_aWXlAC",
        "outputId": "177d832c-c42d-4fd3-972e-23dadf857fea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: |\u001b[43m \u001b[0m: :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dimensões dos espaços\n",
        "\n",
        "Espaço de ações e espaço de estados, respectivamente:\n"
      ],
      "metadata": {
        "id": "ejXnSN4-YkRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyOuyPC2Xn0l",
        "outputId": "cc6e853d-5668-40f7-a6eb-fd37e4f34fc3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(6)\n",
            "Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inicializando a tabela-q\n",
        "\n",
        "Note aqui a razão de precisarmos ter um número finito de estados possíveis. Caso contrário não seria possível mapeá-los em uma tabela\n"
      ],
      "metadata": {
        "id": "i7CHF0OzYuon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tabela_q = np.zeros([env.observation_space.n, env.action_space.n]) #iniciando a tabelo q com zeros"
      ],
      "metadata": {
        "id": "2BS0ggC3Xvil"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    +20 para um desembarque correto.\n",
        "    -10 para um embarque ou desembarque incorreto.\n",
        "    -1 para ações que não sejam as duas anteriores."
      ],
      "metadata": {
        "id": "yMTfiNmCa-mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Treinando o algoritmo"
      ],
      "metadata": {
        "id": "JnxMEKJtY3TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#treinando o algoritmo\n",
        "\n",
        "#aqui não existem valores \"certos\" ou \"errados\", decidimos por tentativa e erro aqueles que otimizaram o treinamento do nosso agente\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1 #determina a chance do agente tomar uma ação aleatória, nesse caso a chance é de 10%\n",
        "\n",
        "for i in range(1, 50001):\n",
        "    estado = env.reset()\n",
        "\n",
        "    epochs, penalidades, recompensa = 0, 0, 0 #epochs é cada episódio\n",
        "    terminado = False\n",
        "    \n",
        "    while not terminado:\n",
        "        if random.uniform(0, 1) < epsilon: #decidindo se será tomado uma ação aleatória ou se seguirá a política da tabela-q\n",
        "            acao = env.action_space.sample() \n",
        "        else:\n",
        "            acao = np.argmax(tabela_q[estado]) \n",
        "\n",
        "        proximo_estado, recompensa, terminado, info = env.step(acao) \n",
        "        \n",
        "        valor_antigo = tabela_q[estado, acao]\n",
        "        proximo_max = np.max(tabela_q[proximo_estado])\n",
        "        \n",
        "        valor_novo = (1 - alpha) * valor_antigo + alpha * (recompensa + gamma * proximo_max) #atualizando o valor de q a partir da equação de Bellman\n",
        "        tabela_q[estado, acao] = valor_novo #colocando este valor na tabela-q\n",
        "\n",
        "        if recompensa == -10: #contabilizando os embarques/desembarques errados\n",
        "            penalidades += 1\n",
        "\n",
        "        estado = proximo_estado\n",
        "        epochs += 1\n",
        "        \n",
        "        clear_output(wait=True) #caso não queira ver o aprendizado comentar as 3 linhas seguintes, essa incluso\n",
        "        env.render()\n",
        "        sleep(.25)  #aumentar se quiser ver melhor o aprendizado (recomendado: .25)\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episódios: {i}\")\n",
        "        #sleep(1)\n",
        "\n",
        "print(\"Treinamento terminado.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kMrwjsOX11E",
        "outputId": "8c9d5ad8-eb12-4803-dcf3-a0a89691c8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testando o agente"
      ],
      "metadata": {
        "id": "TP8HUUKhZBXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testando o algoritmo\n",
        "epochs_totais, penalidades_totais = 0, 0\n",
        "episodios = 100\n",
        "\n",
        "for _ in range(episodios):\n",
        "    estado = env.reset()\n",
        "    epochs, penalidades, recompensa = 0, 0, 0\n",
        "    \n",
        "    terminado = False\n",
        "    \n",
        "    while not terminado:\n",
        "        acao = np.argmax(tabela_q[estado])\n",
        "        estado, recompensa, terminado, info = env.step(acao)\n",
        "\n",
        "        if recompensa == -10:\n",
        "            penalidades += 1\n",
        "\n",
        "        epochs += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        env.render()\n",
        "        sleep(.25)\n",
        "\n",
        "    penalidades_totais += penalidades\n",
        "    epochs_totais += epochs\n",
        "\n",
        "print(f\"Resutados depois de {episodios} episodios:\")\n",
        "print(f\"Média de passos por episódio: {epochs_totais / episodios}\")\n",
        "print(f\"Média de penalidades por episódio: {penalidades_totais / episodios}\")"
      ],
      "metadata": {
        "id": "PJ7iKGoOYL51"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}